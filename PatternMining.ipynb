{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, pickle, glob, os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule mining using ECLAT\n",
    "Notebook taken from a Data Mining course assignment<br>\n",
    "This implementation is my own, but has been influenced by the implementation of Michael Mampaey, available at http://adrem.uantwerpen.be/sites/adrem.ua.ac.be/files/code/eclat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pass:\n",
    "#### Getting the tidlists for the frequent items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext_get_items(files, transaction_attrs, item_attrs, batched=False):\n",
    "    \"\"\"\n",
    "        Returns tidlists of items in the dataframe. \n",
    "        Supports items and transactions with multiple attributes.\n",
    "        Also supports multiple 'types' of items.\n",
    "        Arguments:\n",
    "            * fname: if string, path to data file\n",
    "                     if list, list of paths to data files\n",
    "            * transaction_attrs: list of attributes (as strings) that make up the transactions\n",
    "            * item_attrs: nested list of attributes that make up the item(s). Example: [[\"ID\", \"event\"], [\"cat\", \"event\"]]\n",
    "            * batched: bool, if True, will save tidlists of each file separately & merge them at the end\n",
    "                       (Note: significantly slower, only use when reading more than ~3 files)\n",
    "    \"\"\"\n",
    "    \n",
    "    tidlists = defaultdict(set)\n",
    "    pname = lambda s : s + '-tidlists'\n",
    "    \n",
    "    if type(files) == str:\n",
    "        files = [files]\n",
    "    \n",
    "    for fname in files:\n",
    "        print(f\"Reading file '{fname}'...\")\n",
    "        ftidlists = defaultdict(set) # Write tidlists of this file to separate dict\n",
    "        with open(fname, 'r') as data:\n",
    "            # Read header of file, which gives us column names & indices\n",
    "            columns = data.readline().strip().split(',')\n",
    "            # Read rest of file line by line, collecting items & transactions\n",
    "            for line in data:\n",
    "                line = line.strip().split(',') # list with values on this row\n",
    "                # Collect all items in this row\n",
    "                row_items = list()\n",
    "                for itemtype in item_attrs:\n",
    "                    itemattrs = tuple([line[columns.index(i_at)] for i_at in itemtype])\n",
    "                    row_items.append(frozenset([itemattrs]))\n",
    "                # Find transaction of this row\n",
    "                transaction = frozenset([line[columns.index(t_at)] for t_at in transaction_attrs])\n",
    "\n",
    "                # Add transaction to items\n",
    "                for item in row_items:\n",
    "                    if batched:\n",
    "                        ftidlists[item].add(transaction)\n",
    "                    else:\n",
    "                        tidlists[item].add(transaction)\n",
    "        \n",
    "        # Pickle & store tidlists found in this file\n",
    "        if batched:\n",
    "            with open(pname(fname), 'wb') as pfile:\n",
    "                pickle.dump(ftidlists, pfile)\n",
    "            \n",
    "    if batched:\n",
    "        # Merge all dictionaries into one again\n",
    "        def merge(new, origin={}):\n",
    "            for i in new:\n",
    "                if i in origin:\n",
    "                    origin[i] = origin[i].union(new[i]) # merge tidlists\n",
    "                else:\n",
    "                    origin[i] = new[i]\n",
    "\n",
    "        print(\"Merging tidlists...\")\n",
    "        for fname in files:\n",
    "            with open(pname(fname), 'rb') as pfile:\n",
    "                ftidlists = pickle.load(pfile)\n",
    "                # Merge into existing\n",
    "                merge(ftidlists, tidlists)\n",
    "            \n",
    "    # Convert dict to list\n",
    "    print(\"Finalizing...\")\n",
    "    return list(tidlists.items())\n",
    "\n",
    "\n",
    "def merge_tidlists(fnames):\n",
    "    \"\"\"\n",
    "        Helper function. Just the merging part in the ext_get_items() function, for quicker tidlist loading.\n",
    "    \"\"\"\n",
    "    def merge(new, origin={}):\n",
    "        for i in new:\n",
    "            if i in origin:\n",
    "                origin[i] = origin[i].union(new[i]) # merge tidlists\n",
    "            else:\n",
    "                origin[i] = new[i]\n",
    "\n",
    "    tidlists = defaultdict(set)\n",
    "    print(\"Merging tidlists...\")\n",
    "    for fname in files:\n",
    "        with open(pname(fname), 'rb') as pfile:\n",
    "            ftidlists = pickle.load(pfile)\n",
    "            # Merge into existing\n",
    "            merge(ftidlists, tidlists)\n",
    "    return list(tidlists.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping only the items above minimum support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_items(items, min_sup):\n",
    "    \"\"\"\n",
    "    Prunes non-frequent items, given a minimum support.\n",
    "    Arguments:\n",
    "            * min_sup: int, interpreted as amount of transactions; \n",
    "    \"\"\"\n",
    "    return list(filter(lambda item : len(item[1]) >= min_sup, items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tag-transactions']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = \"./data/tags\"\n",
    "pathlist = [\"tag-transactions\"]#glob.glob(os.path.join(str(datapath), \"*.csv\"))\n",
    "pathlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "transaction_attrs = [\"v_id\"]\n",
    "item_attrs = [[\"tag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tidlists...\n",
      "Reading file 'tag-transactions'...\n",
      "Merging tidlists...\n",
      "Finalizing...\n",
      "Done. Took 0.03 seconds.\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "print(\"Collecting tidlists...\")\n",
    "all_tidlists = ext_get_items(pathlist, transaction_attrs, item_attrs, True)\n",
    "print(f\"Done. Took {time.time()-starttime:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74 frequent items for min. sup. = 7\n"
     ]
    }
   ],
   "source": [
    "MIN_SUP = 7\n",
    "tidlists = prune_items(all_tidlists, MIN_SUP)\n",
    "print(f\"Found {len(tidlists)} frequent items for min. sup. = {MIN_SUP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering certain keywords to reduce scale of search\n",
    "def prune_keywords(tidlists, keywords):\n",
    "    cleaned = list()\n",
    "    for i in tidlists:\n",
    "        item = list(i[0])[0][0]\n",
    "        if item not in keywords:\n",
    "            cleaned.append(i)\n",
    "    return cleaned\n",
    "blacklist = [\"#karen\", \"#karens\", \"#vaccinated\", \"#nissan\", \"#6inch\", \"#4x4\", \"#35s\",\n",
    "             \"#6inch35s\", \"#4x4girls\", \"#np300\", \"#navara\"]\n",
    "cleaned_tidlists = prune_keywords(tidlists, set(blacklist))\n",
    "len(cleaned_tidlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent passes:\n",
    "#### Recursively mining frequent sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_sets(min_sup, tidlists=[], depth=0, max_depth=None, frequents={}):\n",
    "    \"\"\"\n",
    "        Recursively mines frequent itemsets.\n",
    "        Arguments:\n",
    "            * tidlists: list of pairs (itemset, tidlist)\n",
    "            * min_sup: int indicating minimum support\n",
    "        Returns:\n",
    "            Dict of frequent itemsets and their supports\n",
    "    \"\"\"\n",
    "    if max_depth is not None and depth >= max_depth:\n",
    "        return frequents\n",
    "        \n",
    "    # Intersect 'first' items\n",
    "    for i in range(len(tidlists)):\n",
    "        # If first pass, add frequent (single) items\n",
    "        if depth == 0 and len(tidlists[i][1]) >= min_sup:\n",
    "            frequents[tidlists[i][0]] = len(tidlists[i][1])\n",
    "        \n",
    "        # Create conditional database on this item\n",
    "        cond_db = list()\n",
    "        # With all other/next items\n",
    "        for j in range(i+1, len(tidlists)):\n",
    "            # Intersect sets\n",
    "            cand = tidlists[i][0].union(tidlists[j][0])\n",
    "            cand_tids = tidlists[i][1].intersection(tidlists[j][1])\n",
    "                        \n",
    "            # Check support\n",
    "            sup = len(cand_tids)\n",
    "            if sup >= min_sup:\n",
    "                cond_db.append((cand, cand_tids))\n",
    "                frequents[cand] = sup\n",
    "                \n",
    "        # Recursively mine conditional database\n",
    "        i_cond_sets = mine_sets(min_sup, cond_db, depth+1, max_depth, frequents)\n",
    "        \n",
    "    return frequents\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 558 frequent sets in 0.01 seconds.\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "sets = defaultdict(int)\n",
    "sets = mine_sets(MIN_SUP, cleaned_tidlists, frequents=sets)\n",
    "print(f\"Found {len(sets)} frequent sets in {time.time()-starttime:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_counts(sets):\n",
    "    # Returns dict of item counts\n",
    "    counts = defaultdict(int)\n",
    "    for s in sets:\n",
    "        items = [x[0] for x in list(s)]\n",
    "        for i in items:\n",
    "            counts[i] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ics = item_counts(sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#thirdposition 86\n",
      "#based 40\n",
      "#rightwing 66\n",
      "#farright 67\n",
      "#politcs 80\n",
      "#politicstiktok 80\n",
      "#reactionary 80\n",
      "#uk 32\n",
      "#england 80\n",
      "#nationalism 80\n",
      "#traditionalism 50\n",
      "#czech 63\n",
      "#rejectmodernity 34\n",
      "#embracetradition 32\n",
      "#catholic 32\n",
      "#history 48\n",
      "#whitepeoplethings 63\n",
      "#europeforeuropean 64\n",
      "#europeanbeauty 51\n",
      "#whiteculture 60\n",
      "#europeans 59\n",
      "#proudtobewhite 48\n"
     ]
    }
   ],
   "source": [
    "for i in ics:\n",
    "    if ics[i] > 20:\n",
    "        print(i, ics[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Powerset function, copied from Mark Rushakoff at:\n",
    "    https://stackoverflow.com/questions/1482308/how-to-get-all-subsets-of-a-set-powerset\n",
    "\"\"\"\n",
    "from itertools import chain, combinations\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, len(s)+1))\n",
    "\n",
    "\n",
    "def find_rules(freq_items, min_conf):\n",
    "    \"\"\"\n",
    "        Finds rules in the given itemsets.\n",
    "        Arguments:\n",
    "            * freq_items: Dict with itemsets and their confidence\n",
    "            * min_conf: Minimum confidence for a rule.\n",
    "    \"\"\"\n",
    "    rules = list()\n",
    "    for itemset in freq_items:\n",
    "        if len(itemset) == 1:\n",
    "            continue # Skip single frequent items\n",
    "        # Split itemset up into all possible rules\n",
    "        subsets = list(powerset(itemset))\n",
    "        # Check rule from every subset to every other subset\n",
    "        set_conf = freq_items[itemset]\n",
    "        for ss_X in subsets:\n",
    "            ss_X = frozenset(ss_X)\n",
    "            \n",
    "            if len(ss_X) == len(itemset):\n",
    "                continue # Skip rules that have no endpoint\n",
    "            \n",
    "            X_conf = freq_items[ss_X]\n",
    "            rule_conf = float(set_conf) / float(X_conf)\n",
    "            \n",
    "            if rule_conf >= min_conf:\n",
    "                ss_Y = itemset - ss_X\n",
    "                rules.append(((ss_X, ss_Y), rule_conf))\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4434 rules in 0.03 seconds.\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "min_conf = 0.3\n",
    "rules = find_rules(sets, min_conf)\n",
    "print(f\"Found {len(rules)} rules in {time.time()-starttime:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_rules(rules):\n",
    "    \"\"\"\n",
    "        Removes rules that are considered useless.\n",
    "        This is task-specific and requires actual knowledge about the problem;\n",
    "        it could be refined further. In this implementation only some base assumptions have been made.\n",
    "        This implementation also assumes items of the form:\n",
    "        (product ID, event type) or (category ID, event type)\n",
    "    \"\"\"    \n",
    "    def rule_filter(X, eventX, Y, eventY):\n",
    "            # returns bool that indicates whether bool violates filter\n",
    "            return eventX in X and eventY in Y\n",
    "        \n",
    "    def filter_terms(terms, X, Y):\n",
    "        # True if any of the terms appear in either LHS (X) or RHS (Y) of the rule\n",
    "        return len(set(terms) & (set(X + Y))) > 0\n",
    "    \n",
    "    #to_del = set() # List of rules to remove\n",
    "    cleaned = list() # List of rules to keep\n",
    "    for r in rules:\n",
    "        X = [x[0] for x in list(r[0][0])]\n",
    "        Y = [x[0] for x in list(r[0][1])]\n",
    "        # Removes rules with 'karen' or 'karens' in them, since they most likely are satirical\n",
    "        rem_flag = filter_terms([\"#karen\", \"#karens\", \"#vaccinated\"], X, Y)\n",
    "        if rem_flag:\n",
    "            #to_del.add(r)\n",
    "            continue\n",
    "        \n",
    "        \"\"\"\n",
    "        # Removes rules of type: 'remove -> cart', 'cart -> view', 'purchase -> cart'\n",
    "        ev_flag = event_filter(X, \"remove_from_cart\", Y, \"cart\") \\\n",
    "                or event_filter(X, \"cart\", Y, \"view\") \\\n",
    "                or event_filter(X, \"purchase\", Y, \"cart\")\n",
    "        if ev_flag:\n",
    "            to_del.add(r)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Heuristically removing 'obvious rules': if conf is too high, it's likely\n",
    "        # something like \"product of category X in cart -> category X in cart\"\n",
    "        conf = r[1]\n",
    "        max_conf = 0.99\n",
    "        if conf > max_conf:\n",
    "            #to_del.add(r)\n",
    "            continue\n",
    "        \n",
    "        # If all filters passed, keep rule\n",
    "        cleaned.append(r)\n",
    "\n",
    "    #return list(filter(lambda r : r not in to_del, rules))\n",
    "    return cleaned\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 2880 rules. Took 0.01 seconds.\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "final_rules = prune_rules(rules)\n",
    "print(f\"Kept {len(final_rules)} rules. Took {time.time()-starttime:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule:\n",
      "{('#history',), ('#europeans',), ('#whiteculture',)} => {('#europeanbeauty',)} : 0.70\n",
      "Rule:\n",
      "{('#republican',)} => {('#democrat',)} : 0.70\n",
      "Rule:\n",
      "{('#europeforeuropean',), ('#europeans',)} => {('#proudtobewhite',), ('#whitepeoplethings',)} : 0.70\n",
      "Rule:\n",
      "{('#based',), ('#authright',)} => {('#chad',)} : 0.69\n",
      "Rule:\n",
      "{('#based',), ('#authright',)} => {('#nationalism',)} : 0.69\n",
      "Rule:\n",
      "{('#based',), ('#czech',)} => {('#embracetradition',)} : 0.69\n",
      "Rule:\n",
      "{('#based',), ('#czech',)} => {('#rejectmodernity',), ('#embracetradition',)} : 0.69\n",
      "Rule:\n",
      "{('#slav',)} => {('#catholic',), ('#nationalism',)} : 0.69\n",
      "Rule:\n",
      "{('#slav',)} => {('#nationalism',), ('#czech',)} : 0.69\n",
      "Rule:\n",
      "{('#europeanbeauty',), ('#europeans',), ('#whiteculture',)} => {('#proudtobewhite',), ('#whitepeoplethings',), ('#europeforeuropean',)} : 0.69\n",
      "Rule:\n",
      "{('#europeanbeauty',), ('#europeforeuropean',), ('#europeans',), ('#whiteculture',)} => {('#proudtobewhite',), ('#whitepeoplethings',)} : 0.69\n",
      "Rule:\n",
      "{('#europeanbeauty',), ('#whitepeoplethings',), ('#europeforeuropean',), ('#europeans',)} => {('#proudtobewhite',), ('#whiteculture',)} : 0.69\n",
      "Rule:\n",
      "{('#europeanbeauty',), ('#europeans',), ('#whiteculture',)} => {('#proudtobewhite',), ('#whitepeoplethings',)} : 0.69\n",
      "Rule:\n",
      "{('#orthodox',), ('#nationalism',)} => {('#traditionalism',), ('#catholic',), ('#czech',)} : 0.69\n",
      "Rule:\n",
      "{('#traditionalism',), ('#catholic',), ('#czech',)} => {('#orthodox',), ('#nationalism',)} : 0.69\n",
      "Rule:\n",
      "{('#traditionalism',), ('#nationalism',), ('#catholic',), ('#czech',)} => {('#orthodox',)} : 0.69\n",
      "Rule:\n",
      "{('#rejectmodernity',), ('#nationalism',)} => {('#traditionalism',), ('#czech',)} : 0.69\n",
      "Rule:\n",
      "{('#nationalism',), ('#embracetradition',)} => {('#traditionalism',), ('#czech',)} : 0.69\n",
      "Rule:\n",
      "{('#rejectmodernity',), ('#nationalism',)} => {('#traditionalism',), ('#embracetradition',), ('#czech',)} : 0.69\n",
      "Rule:\n",
      "{('#nationalism',), ('#embracetradition',)} => {('#traditionalism',), ('#rejectmodernity',), ('#czech',)} : 0.69\n"
     ]
    }
   ],
   "source": [
    "final_rules.sort(key=lambda r : r[1], reverse=True)\n",
    "k = 1000\n",
    "for r in final_rules[k:k+20]:\n",
    "    print(\"Rule:\")\n",
    "    print(f\"{set(r[0][0])} => {set(r[0][1])} : {r[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
